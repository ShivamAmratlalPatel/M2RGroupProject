@article{robbins1952some,
    title = {Some aspects of the sequential design of experiments},
    author = {Robbins, Herbert},
    journal = {Bulletin of the American Mathematical Society},
    volume = {58},
    number = {5},
    pages = {527--535},
    year = {1952},
    publisher = {Citeseer}
}

@inproceedings{chen2013combinatorial,
    title = {Combinatorial multi-armed bandit: General framework and applications},
    author = {Chen, Wei and Wang, Yajun and Yuan, Yang},
    booktitle = {International Conference on Machine Learning},
    pages = {151--159},
    year = {2013},
    organization = {PMLR}
}

@article{bouneffouf2019survey,
    author = {Djallel Bouneffouf and
 Irina Rish},
    title = {A Survey on Practical Applications of Multi-Armed and Contextual Bandits},
    journal = {CoRR},
    volume = {abs/1904.10040},
    year = {2019},
    url = {http://arxiv.org/abs/1904.10040},
    archivePrefix = {arXiv},
    eprint = {1904.10040},
    timestamp = {Mon, 04 Nov 2019 12:36:13 +0100},
    biburl = {https://dblp.org/rec/journals/corr/abs-1904-10040.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{vermorel,
    author = "Vermorel, Joann{\`e}s
and Mohri, Mehryar",
    editor = "Gama, Jo{\~a}o
and Camacho, Rui
and Brazdil, Pavel B.
and Jorge, Al{\'i}pio M{\'a}rio
and Torgo, Lu{\'i}s",
    title = "Multi-armed Bandit Algorithms and Empirical Evaluation",
    booktitle = "Machine Learning: ECML 2005",
    year = "2005",
    publisher = "Springer Berlin Heidelberg",
    address = "Berlin, Heidelberg",
    pages = "437--448",
    abstract = "The multi-armed bandit problem for a gambler is to decide which arm of a K-slot machine to pull to maximize his total reward in a series of trials. Many real-world learning and optimization problems can be modeled in this way. Several strategies or algorithms have been proposed as a solution to this problem in the last two decades, but, to our knowledge, there has been no common evaluation of these algorithms.",
    isbn = "978-3-540-31692-3"
}

@article{chapelle2011empirical,
    title = {An empirical evaluation of thompson sampling},
    author = {Chapelle, Olivier and Li, Lihong},
    journal = {Advances in neural information processing systems},
    volume = {24},
    pages = {2249--2257},
    year = {2011},
    publisher = {Citeseer}
}

@article{DBLP:journals/corr/abs-1807-09809,
    author = {Mark Collier and
 Hector Urdiales Llorens},
    title = {Deep Contextual Multi-armed Bandits},
    journal = {CoRR},
    volume = {abs/1807.09809},
    year = {2018},
    url = {http://arxiv.org/abs/1807.09809},
    archivePrefix = {arXiv},
    eprint = {1807.09809},
    timestamp = {Mon, 13 Aug 2018 16:47:01 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-1807-09809.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{akanmu2019towards,
    title = {Towards an Improved Strategy for Solving Multi-Armed Bandit Problem},
    author = {Akanmu, S and Garg, Rakhen and Gilal, A},
    journal = {International Journal of Innovative Technology and Exploring Engineering (IJITEE)},
    volume = {8},
    number = {12},
    year = {2019}
}

@article{SVGarbar2012,
    author = {S{\'{e}}bastien Bubeck and
 Nicol{\`{o}} Cesa{-}Bianchi},
    title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit
 Problems},
    journal = {CoRR},
    volume = {abs/1204.5721},
    year = {2012},
    url = {http://arxiv.org/abs/1204.5721},
    archivePrefix = {arXiv},
    eprint = {1204.5721},
    timestamp = {Mon, 13 Aug 2018 16:49:06 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-1204-5721.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Inbook{Hoeffding1963,
    author = "Hoeffding, Wassily",
    editor = "Fisher, N. I.
and Sen, P. K.",
    title = "Probability Inequalities for sums of Bounded Random Variables",
    bookTitle = "The Collected Works of Wassily Hoeffding",
    year = "1994",
    publisher = "Springer New York",
    address = "New York, NY",
    pages = "409--426",
    abstract = "Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for PrS --- ES≥nt depend only on the endpoints of the ranges of the summands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population.",
    isbn = "978-1-4612-0865-5",
    doi = "10.1007/978-1-4612-0865-5_26",
    url = "https://doi.org/10.1007/978-1-4612-0865-5_26"
}

@article{Auer2002,
    title = {Finite-time analysis of the multiarmed bandit problem},
    author = {Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
    journal = {Machine learning},
    volume = {47},
    number = {2},
    pages = {235--256},
    year = {2002},
    publisher = {Springer}
}

@InProceedings{Garivier2011,
title = {The KL-UCB algorithm for bounded stochastic bandits and beyond},
author = {Garivier, Aurélien, and Olivier Cappé},
booktitle = {24th annual conference on learning theory},
publisher = {JMLR Workshop and Conference Proceedings},
year = {2011}
}


@article{Chan_2020,
author = {Hock Peng Chan},
title = {{The multi-armed bandit problem: An efficient nonparametric solution}},
volume = {48},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {346 -- 373},
abstract = {Lai and Robbins (Adv. in Appl. Math. 6 (1985) 4–22) and Lai (Ann. Statist. 15 (1987) 1091–1114) provided efficient parametric solutions to the multi-armed bandit problem, showing that arm allocation via upper confidence bounds (UCB) achieves minimum regret. These bounds are constructed from the Kullback–Leibler information of the reward distributions, estimated from specified parametric families. In recent years, there has been renewed interest in the multi-armed bandit problem due to new applications in machine learning algorithms and data analytics. Nonparametric arm allocation procedures like $\epsilon $-greedy, Boltzmann exploration and BESA were studied, and modified versions of the UCB procedure were also analyzed under nonparametric settings. However, unlike UCB these nonparametric procedures are not efficient under general parametric settings. In this paper, we propose efficient nonparametric procedures.},
keywords = {efficiency, KL-UCB, subsampling, Thompson sampling, UCB},
year = {2020},
doi = {10.1214/19-AOS1809},
URL = {https://doi.org/10.1214/19-AOS1809}
}

@InProceedings{agrawal2012analysis, title = {Analysis of Thompson Sampling for the Multi-armed Bandit Problem}, author = {Shipra Agrawal and Navin Goyal}, booktitle = {Proceedings of the 25th Annual Conference on Learning Theory}, pages = {39.1--39.26}, year = {2012}, editor = {Shie Mannor and Nathan Srebro and Robert C. Williamson}, volume = {23}, series = {Proceedings of Machine Learning Research}, address = {Edinburgh, Scotland}, month = {25--27 Jun}, publisher = {JMLR Workshop and Conference Proceedings}, pdf = {http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf}, url = {http://proceedings.mlr.press/v23/agrawal12.html}, abstract = {The multi-armed bandit problem is a popular model for studying exploration/exploitation trade-off in sequential decision problems. Many algorithms are now available for this well-studied problem. One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. This algorithm, referred to as Thompson Sampling, is a natural Bayesian algorithm. The basic idea is to choose an arm to play according to its probability of being the best arm. Thompson Sampling algorithm has experimentally been shown to be close to optimal. In addition, it is efficient to implement and exhibits several desirable properties such as small regret for delayed feedback. However, theoretical understanding of this algorithm was quite limited. In this paper, for the first time, we show that Thompson Sampling algorithm achieves logarithmic expected regret for the stochastic multi-armed bandit problem. More precisely, for the stochastic two-armed bandit problem, the expected regret in time T is O(\frac\ln T∆ + \frac1∆^3). And, for the stochastic N-armed bandit problem, the expected regret in time T is O(\left[\left(\sum_i=2^N \frac1\Delta_i^2\right)^2\right] \ln T). Our bounds are optimal but for the dependence on \Delta_i and the constant factors in big-Oh.} }

@InProceedings{pmlr-v37-komiyama15, title = {Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed Bandit Problem with Multiple Plays}, author = {Komiyama, Junpei and Honda, Junya and Nakagawa, Hiroshi}, booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, pages = {1152--1161}, year = {2015}, editor = {Bach, Francis and Blei, David}, volume = {37}, series = {Proceedings of Machine Learning Research}, address = {Lille, France}, month = {07--09 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v37/komiyama15.pdf}, url = { http://proceedings.mlr.press/v37/komiyama15.html }, abstract = {We discuss a multiple-play multi-armed bandit (MAB) problem in which several arms are selected at each round. Recently, Thompson sampling (TS), a randomized algorithm with a Bayesian spirit, has attracted much attention for its empirically excellent performance, and it is revealed to have an optimal regret bound in the standard single-play MAB problem. In this paper, we propose the multiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the multiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS has the optimal regret upper bound that matches the regret lower bound provided by Anantharam et al.\,(1987). Therefore, MP-TS is the first computationally efficient algorithm with optimal regret. A set of computer simulations was also conducted, which compared MP-TS with state-of-the-art algorithms. We also propose a modification of MP-TS, which is shown to have better empirical performance.} }

@InProceedings{agrawal2013further,
title = {Further Optimal Regret Bounds for Thompson Sampling},
author = {Agrawal, Shipra and Goyal, Navin},
booktitle = {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
pages = {99--107},
year = {2013},
editor = {Carvalho, Carlos M. and Ravikumar, Pradeep},
volume = {31},
series = {Proceedings of Machine Learning Research},
address = {Scottsdale, Arizona, USA},
month = {29 Apr--01 May},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v31/agrawal13a.pdf},
url = {http://proceedings.mlr.press/v31/agrawal13a.html},
abstract = {Thompson Sampling is one of the oldest heuristics for multi-armed bandit problems. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have comparable or better empirical performance compared to the state of the art methods. In this paper, we provide a novel regret analysis for Thompson Sampling that proves the first near-optimal problem-independent bound of O(\sqrtNT\ln T) on the expected regret of this algorithm. Our novel martingale-based analysis techniques are conceptually simple, and easily extend to distributions other than the Beta distribution. For the version of Thompson Sampling that uses Gaussian priors, we prove a problem-independent bound of O(\sqrtNT\ln N) on the expected regret, and demonstrate the optimality of this bound by providing a matching lower bound. This lower bound of Ω(\sqrtNT\ln N) is the first lower bound on the performance of a natural version of Thompson Sampling that is away from the general lower bound of O(\sqrtNT) for the multi-armed bandit problem. Our near-optimal problem-independent bounds for Thompson Sampling solve a COLT 2012 open problem of Chapelle and Li. Additionally, our techniques simultaneously provide the optimal problem-dependent bound of (1+ε)\sum_i \frac\ln Td(\mu_i, \mu_1)+O(\fracNε^2) on the expected regret. The optimal problem-dependent regret bound for this problem was first proven recently by Kaufmann et al. [2012].}
}

@article{slivkins2019introduction,
author = {Aleksandrs Slivkins},
title = {Introduction to Multi-Armed Bandits},
journal = {CoRR},
volume = {abs/1904.07272},
year = {2019},
url = {http://arxiv.org/abs/1904.07272},
archivePrefix = {arXiv},
eprint = {1904.07272},
timestamp = {Thu, 25 Apr 2019 13:55:01 +0200},
biburl = {https://dblp.org/rec/journals/corr/abs-1904-07272.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{allesiardo2017non,
title = {The non-stationary stochastic multi-armed bandit problem},
author = {Allesiardo, Robin and F{\'e}raud, Rapha{\"e}l and Maillard, Odalric-Ambrym},
journal = {International Journal of Data Science and Analytics},
volume = {3},
number = {4},
pages = {267--283},
year = {2017},
publisher = {Springer}
}

@article{mazumdar2020thompson,
author = {Eric Mazumdar and
 Aldo Pacchiano and
 Yi{-}An Ma and
 Peter L. Bartlett and
 Michael I. Jordan},
title = {On Thompson Sampling with Langevin Algorithms},
journal = {CoRR},
volume = {abs/2002.10002},
year = {2020},
url = {https://arxiv.org/abs/2002.10002},
archivePrefix = {arXiv},
eprint = {2002.10002},
timestamp = {Tue, 03 Mar 2020 14:32:13 +0100},
biburl = {https://dblp.org/rec/journals/corr/abs-2002-10002.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kuleshov,
author = {Volodymyr Kuleshov and
 Doina Precup},
title = {Algorithms for multi-armed bandit problems},
journal = {CoRR},
volume = {abs/1402.6028},
year = {2014},
url = {http://arxiv.org/abs/1402.6028},
archivePrefix = {arXiv},
eprint = {1402.6028},
timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
biburl = {https://dblp.org/rec/journals/corr/KuleshovP14.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{zhou2018racing,
title = {Racing Thompson: an Efficient Algorithm for Thompson Sampling with Non-conjugate Priors},
author = {Zhou, Yichi and Zhu, Jun and Zhuo, Jingwei},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
pages = {6000--6008},
year = {2018},
editor = {Dy, Jennifer and Krause, Andreas},
volume = {80},
series = {Proceedings of Machine Learning Research},
month = {10--15 Jul},
publisher = {PMLR},
pdf = {http://proceedings.mlr.press/v80/zhou18e/zhou18e.pdf},
url = {http://proceedings.mlr.press/v80/zhou18e.html},
abstract = {Thompson sampling has impressive empirical performance for many multi-armed bandit problems. But current algorithms for Thompson sampling only work for the case of conjugate priors since they require to perform online Bayesian posterior inference, which is a difficult task when the prior is not conjugate. In this paper, we propose a novel algorithm for Thompson sampling which only requires to draw samples from a tractable proposal distribution. So our algorithm is efficient even when the prior is non-conjugate. To do this, we reformulate Thompson sampling as an optimization proplem via the Gumbel-Max trick. After that we construct a set of random variables and our goal is to identify the one with highest mean which is an instance of best arm identification problems. Finally, we solve it with techniques in best arm identification. Experiments show that our algorithm works well in practice.}
}

@inproceedings{kocsis2006discounted,
title = {Discounted ucb},
author = {Kocsis, Levente and Szepesv{\'a}ri, Csaba},
booktitle = {2nd PASCAL Challenges Workshop},
volume = {2},
year = {2006}
}

@misc{garivier2008upper,
title ={On Upper-Confidence Bound Policies for Non-Stationary Bandit Problems},
author ={Aurélien Garivier and Eric Moulines},
year ={2008},
eprint ={0805.3415},
archivePrefix ={arXiv},
primaryClass ={math.ST}
}

@misc{besson2020efficient,
title ={Efficient Change-Point Detection for Tackling Piecewise-Stationary Bandits},
author ={Lilian Besson and Emilie Kaufmann and Odalric-Ambrym Maillard and Julien Seznec},
year ={2020},
eprint ={1902.01575},
archivePrefix ={arXiv},
primaryClass ={stat.ML}
}

@ARTICLE{2020NumPy-Array,
author = {Harris, Charles R. and Millman, K. Jarrod and
 van der Walt, Stéfan J and Gommers, Ralf and
 Virtanen, Pauli and Cournapeau, David and
 Wieser, Eric and Taylor, Julian and Berg, Sebastian and
 Smith, Nathaniel J. and Kern, Robert and Picus, Matti and
 Hoyer, Stephan and van Kerkwijk, Marten H. and
 Brett, Matthew and Haldane, Allan and
 Fernández del Río, Jaime and Wiebe, Mark and
 Peterson, Pearu and Gérard-Marchant, Pierre and
 Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and
 Abbasi, Hameer and Gohlke, Christoph and
 Oliphant, Travis E.},
title = {Array programming with {NumPy}},
journal = {Nature},
year = {2020},
volume = {585},
pages = {357–362},
doi = {10.1038/s41586-020-2649-2}
}

@Article{Hunter:2007,
Author = {Hunter, J. D.},
Title = {Matplotlib: A 2D graphics environment},
Journal = {Computing in Science \& Engineering},
Volume = {9},
Number = {3},
Pages = {90--95},
abstract = {Matplotlib is a 2D graphics package used for Python for
 application development, interactive scripting, and publication-quality
 image generation across user interfaces and operating systems.},
publisher = {IEEE COMPUTER SOC},
doi = {10.1109/MCSE.2007.55},
year = 2007
}

@book{10.5555/1593511,
author = {Van Rossum, Guido and Drake, Fred L.},
title = {Python 3 Reference Manual},
year = {2009},
isbn = {1441412697},
publisher = {CreateSpace},
address = {Scotts Valley, CA}
}

@conference{Kluyver2016jupyter,
Title = {Jupyter Notebooks -- a publishing format for reproducible computational workflows},
Author = {Thomas Kluyver and Benjamin Ragan-Kelley and Fernando P{\'e}rez and Brian Granger and Matthias Bussonnier and Jonathan Frederic and Kyle Kelley and Jessica Hamrick and Jason Grout and Sylvain Corlay and Paul Ivanov and Dami{\'a}n Avila and Safia Abdalla and Carol Willing},
Booktitle = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
Editor = {F. Loizides and B. Schmidt},
Organization = {IOS Press},
Pages = {87 - 90},
Year = {2016}
}
@ARTICLE{2020SciPy-NMeth,
author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
 Haberland, Matt and Reddy, Tyler and Cournapeau, David and
 Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
 Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
 Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
 Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
 Kern, Robert and Larson, Eric and Carey, C J and
 Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
 Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
 Harris, Charles R. and Archibald, Anne M. and
 Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
title = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
 Computing in Python}},
journal = {Nature Methods},
year = {2020},
volume = {17},
pages = {261--272},
adsurl = {https://rdcu.be/b08Wh},
doi = {10.1038/s41592-019-0686-2},
}

@article{Liu_Lee_Shroff_2018,
title ={A Change-Detection Based Framework for Piecewise-Stationary Multi-Armed Bandit Problem},
volume ={32}, url ={https://ojs.aaai.org/index.php/AAAI/article/view/11746},
abstractNote ={ &lt;p&gt; The multi-armed bandit problem has been extensively studied under the stationary assumption.
However in reality, this assumption often does not hold because the distributions of rewards themselves may change over time.
In this paper, we propose a change-detection (CD) based framework for multi-armed bandit problems under the piecewise-stationary setting, and study a class of change-detection based UCB (Upper Confidence Bound) policies, CD-UCB, that actively detects change points and restarts the UCB indices.
We then develop CUSUM-UCB and PHT-UCB, that belong to the CD-UCB class and use cumulative sum (CUSUM) and Page-Hinkley Test (PHT) to detect changes.
We show that CUSUM-UCB obtains the best known regret upper bound under mild assumptions.
We also demonstrate the regret reduction of the CD-UCB policies over arbitrary Bernoulli rewards and Yahoo! datasets of webpage click-through rates. &lt;/p&gt; },
number ={1},
journal ={Proceedings of the AAAI Conference on Artificial Intelligence},
author ={Liu, Fang and Lee, Joohyun and Shroff, Ness},
year ={2018},
month ={Apr.} }

@INPROCEEDINGS{wei2018abruptly,
author ={Wei, Lai and Srivatsva, Vaibhav},
booktitle ={2018 Annual American Control Conference (ACC)},
title ={On Abruptly-Changing and Slowly-Varying Multiarmed Bandit Problems},
year ={2018},
pages ={6291-6296},
doi ={10.23919/ACC.2018.8431265}}
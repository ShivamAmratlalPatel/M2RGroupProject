\section{Stationary Strategies}\label{sec:stationary-strategies}
Here, we define \textbf{stationary} distributions for arms as those where the underlying distribution of each arm stays the same regardless of the round we are in - some strategies for this type of multi-armed bandit problem are described below.

\subsection{$\epsilon$-Greedy}\label{subsec: $epsilon$-greedy}
    The $\epsilon$-Greedy strategy is where arms are 'explored randomly $\epsilon$ percentage of rounds and exploited $1-\epsilon$ percentage of the time' \citep{DBLP:journals/corr/abs-1807-09809}.
    It follows that the $\epsilon$ we choose must be in the interval [0,1], and we would like to choose $\epsilon$ so that we get the perfect balance between exploration (gathering data from all arms) and exploitation (playing the arm with the highest realised expectation).
    This theme is key to all solution strategies of the multi-armed bandit, but later strategies, rather than having such a clear divide between exploration and exploitation, have more of a dynamic aproach.

    \subsubsection{Random strategy}
    The first (and most basic) of the $\epsilon$-Greedy strategies is the random strategy, which will provide us a benchmark against which to compare all other strategies.
    As the name implies, for each round we choose an arm at random.
    This means $\epsilon=1$ and there will be pure exploration and no exploitation.

    \subsubsection{Epsilon-first Strategy}

    The $\epsilon$-first strategy consists of doing the exploration all at once at the beginning \citep{vermorel}.
    Assume we arbitrarily take k trails in total, where k $\in \mathbb{N}$, then $\epsilon$k trails of them will be selected in the exploration phase, where $\epsilon$ is in the interval (0,1), during which an arm is randomly selected.
    This is followed by (1-$\epsilon$)k trails in the exploitation phase, where the arm with the highest realised expectation is always selected.
    \par
    \textbf{Pseudo Code for Epsilon-first Strategy}
    \newline
    \begin{algorithm}[H]
        \KwData{number of arms, number of arms, epsilon}
        \KwResult{regret after using epsilon strategy\;}
        \For{number of arms*epsilon}{
            Pick random arm\;
        }
        Calculate realised expectation of each arm\;

        \For{number of arms*(1-epsilon)}{
            Pick arm with highest realised expectation\;
        }
        Calculate and return regret
        \caption{Epsilon first strategy}\label{alg:epsilon_algorithm}
    \end{algorithm}

    \subsection{Upper Confidence Bound}\label{subsec:upper-confidence-bound}
    In the Upper Confidence Bound (UCB) strategy, we use the realised values to construct upper confidence bounds on the expectation of each arm at some fixed confidence level.
    We then choose the arm which has the highest upper confidence bound to pull next~\citep{SVGarbar2012}. As an arm is pulled more its confidence region gets smaller and our estimate is assumed to be more accurate.
    \par
    The algorithm measures potential by an upper confidence bound of the reward value, $\widehat{U_t (a)}$.
    Then the true value becomes $ Q(a)  \leq \widehat{Q_t (a)}+\widehat{U_t (a)} $, where  $ \widehat{Q_t (a)}$ is the sample mean and $ Q(a)$ is the true mean.

    \subsubsection{Hoeffdingâ€™s Inequality}
    Hoeffding's inequality is applicable in this strategy.
    Let $X_1, \dots, X_n$ be independent and identically distributed random variables, then $ P(E[X] > \overline  X_t + u) \leq e^{-2t u^2}$ with $u$ being the upper confidence bound and $ \overline X_t $ the sample mean respectively. \citep{Hoeffding1963}
    \par
    Applied in our case, 
    $$P[Q(a) >\widehat{Q_t (a)}+\widehat{U_t (a)}] \leq e^{-2t U_t(a)^2}$$
    ,where $a$ represents the arm index and $t$ is the time. We take $e^{-2t U_t(a)^2} $ to be the threshold here and represented by $p$.
    So we can get $U_t(a) = \sqrt{\frac{-\log p}{2N_t (a)}}$, where $N_t (a)$ is the number of times arm $a$ has been played at time $t$.
    \par
    The threshold is used as the upper bound for probability.
    We can use Hoeffding's inequality to get the confidence bounds of reward variance.

    \subsubsection{UCB1}
% This sentence does not make gramamtical sense but I don't understand what it is and what you are trying to say so do not have any suggestions. Also make sure to use English (UK) spelling and not English (US) spelling. For example initialisation instead of initialization and maximises instead of maximizes. I think you should highlight more the difference between UCB and UCB1.
    UCB1 uses Hoeffding's inequality as the upper limit of the average reward distribution of the arm, where the true average is likely to be lower than the UCB allocated by the algorithm.
    UCB1, achieving logarithmic regret uniformly over $n$ and without any preliminary knowledge about the reward distributions. The index of this policy is the sum of two terms.
    The strategy starts by playing each arm once, we do the loop that play arm $t$ which maximises $\overline X_t + \sqrt{\frac{2log t}{N_t (a)}}$~\citep{Auer2002}.
    \par
    Under UCB1, logarithmic regret $O(\log N)$  is achieved when the reward distributions are supported on [0,1] \citep{Chan_2020}.
    We can conclude that regret of UCB1 grows at a rate of $\log N$.
    \par
    \subsubsection{KL-UCB}
    In KL-UCB strategy, we first pull each arm once and choose the next pull according to regret bounds rely on deviations results of independent interest. \citep{Garivier2011}
    \par
    The regret of KL-UCB algorithm is $$\lim_{n\to \infty} \frac{E[R_n]}{\log n} \leq  \sum_{a:u_a < u_{a*}} \frac{u_{a*}-u_a}{d(u_a,u_{a*})}  $$ 
    where $d(p, q) = p \log(p/q) + (1 - p) * \log((1 - p/(1 - q))$ denotes the Kullback-Leibler diver- gence between Bernoulli distributions of parameters p and q, respectively. Here K is the number of arms, a* is the optimal arm and independent rewards bounded in [0,1].\citep{Garivier2011}
    \par
    \textbf{Pseudocode for Upper Confidence Bound Strategy}
    \newline
    \begin{algorithm}[H]
        \KwData{number of arms, number of arms, confidence level}
        \KwResult{regret after using UCB strategy\;}
        \For{number of arms}{
            Pick arm with highest upper confidence bound\;
            arm uncertainty = confidence level $\times\sqrt{\frac{\ln{N+1}}{N}}$\;
            arm upper confidence bound = arm realised expectation + arm uncertainty\;
        }
        Calculate and return regret
        \caption{UCB Strategy}\label{alg:ucb_algorithm}
    \end{algorithm}

    \subsection{Thompson Sampling (Bayesian)}\label{subsec:thompson-sampling-(bayesian)}
    The goal of Thompson sampling is to construct an accurate estimate of rewards by building a \textbf{probability distribution} based on the rewards we have already received - this is an example of \textbf{Bayesian inference}, where we update our models based on new knowledge.
    More specifically, we construct the \textbf{conjugate prior} for the distributions of the arms allowing us to approximately predict our posterior reward distribution.
    \par
    Using these prior distributions, we choose the next arm to play by sampling from the prior distribution of each arm, selecting the highest value from the samples, and playing the arm which gives this value;
    we repeat this method after each round, changing our conjugate prior distributions for the following round based on new information we gain.
    These conjugate prior distributions will continue to change as we gain more information (and therefore confidence) about each arm's distribution.
    In this section, we shall go through the Thompson sampling methods where the distribution of the arms is different in each case~\citep{agrawal2012analysis}.

    \subsubsection{Bernoulli Thompson Sampling}
    As the name suggests, in this case, all the arms have a Bernoulli distribution: therefore we can model our conjugate prior distribution as a Beta($\alpha$,$\beta$) distribution, where $\alpha$ corresponds to the number of successes (rounds where the arm gives one), and $\beta$ corresponds to the number of failures (rounds where the arm gives zero). As we have no previous successes or failures in the first round, we shall set both $\alpha$ and $\beta$ to 1, and after this, we sample using the constructed Beta distribution for each arm, with $\alpha$ and $\beta$ being updated for each arm the more we play. As we go on, this results in the distributions with higher $\alpha$ values and lower $\beta$ values being selected more often, maximising our reward.

    \subsubsection{Gaussian Thompson Sampling}
    In this case, the arms have a Gaussian (Normal) distribution \textbf{with known variance}: this means we can model our conjugate prior distribution as a Normal distribution.
    However, the parameters are now changed.
    We shall use the \textbf{precision} ($\tau$), which is 1/variance and therefore known, to make the expression to update the parameters after each round easier to manipulate.
    We name our modelling parameters for the arm before the round $\mu_0$ and $\tau_0$ for the mean and precision respectively.
    Therefore, the updated precision for the arm after the round is given by
    \[\tau_0\xleftarrow{}\tau_0 + n\tau\text{,}\]
    and the updated mean for the arm after the round is given by
    \[\mu_0\xleftarrow{}\frac{\tau_0\mu_0 + \tau\sum_{t=1}^n\widehat{r}_t}{\tau_0 + n\tau}\text{,}\]
    where $\widehat{r}_t$ is the reward for each round the arm has been tested in \citep{agrawal2013further} (which was defined in section 1.3), and n is the number of times the arm has been played. This means the posterior distribution in this case is a normal distribution using the updated parameters above for the precision and mean respectively;
    this will function as our prior distribution for the next round.
    Similar to our Bernoulli Thompson sampling model above, as the rounds progress, the precision for each arm increases, and the variance for each arm therefore decreases, meaning that the arms with the highest estimated mean are chosen more often.

    \subsubsection{Regret for Thompson Sampling}
    The expected regret for both our Thompson sampling models above has order
    $$O(\sqrt{NT\ln{T}})$$
    where T is the number of rounds and N is the number of arms~\citep{agrawal2013further}. This means that Thompson sampling is a \textbf{zero regret} strategy, as the order of the expected regret is such that its gradient becomes flatter as time progresses. This is because as $T\to\infty$, $NT\ln{T} < NT^2$ (with N being a constant), and the gradient of the square root of $NT^2$ is constant, meaning the gradient of the expected regret decreases.
    \newline
    \textbf{Pseudocode for Thompson Sampling Strategy}
    \newline
    \begin{algorithm}[H]
        \KwData{number of arms, number of arms, confidence level}
        \KwResult{regret after using Thompson Sampling strategy\;}
        Set all arms with prior distribution with a uniform distribution\;
        \For{number of arms}{
            Sample distributions for all arms\;
            Select arm with highest arg max of sample
            Update success and failure numbers of arm
        }
        Calculate and return regret
        \caption{Thompson Strategy}\label{alg:thompson_algorithm}
    \end{algorithm}
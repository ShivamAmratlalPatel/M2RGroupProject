\section{Strengths and Weaknesses}\label{sec:strengths-and-weaknesses}
In this section, we explore which strategies work best and worst in different scenarios, whilst also comparing them to each other.

\subsection{$\epsilon$-Greedy}\label{subsec:$epsilon$-greedy}

\subsubsection{Strengths}
The largest strength of epsilon-greedy is it is quite easy to understand and it is not computationally expensive at all, especially compared to UCB and Thompson Sampling - we have found that it has been the easiest strategy to code in our Python environment.
After an initial period of exploration (for example 10,000 rounds), the algorithm greedily exploits the best option k, $\epsilon$ percent of the time.
For example, if we set $\epsilon$=0.05, the algorithm will exploit the best variant 95\% of the time and will explore random alternatives 5\% of the time.
This is actually quite effective in practice.

\subsubsection{Weaknesses}
Epsilon-Greedy is a pretty powerful algorithm capable of exploring the arms and making close to optimal decisions within it.
However, the algorithm does not always identify the absolute best arm.
Certainly we can increase the rounds for exploration, but then we are wasting more rounds randomly searching, which will further hurt the total reward.
Also, there is inherent randomness in this process, so if we were to run the algorithm again it is possible that epsilon-greedy will identify and exploit the best arm.
This means that epsilon-greedy can get stuck exploiting a sub-optimal arm.
\newline
In addition, the Epsilon-Greedy strategy's regret will scale linearly, due to a random arm being chosen every so often (this will probably not be the optimal arm in later rounds).
Therefore, this strategy will never be a zero-regret strategy, unlike UCB and Thompson sampling strategies.

\subsubsection{Best conditions for $\epsilon$-Greedy algorithm}
It is clear to see from the previous sections that the $\epsilon$-Greedy algorithm is far less computationally expensive than the UCB and Thompson Sampling algorithms. 
Therefore, we can conclude that when we don't have very powerful computers to process algorithms, $\epsilon$-Greedy will be the optimal choice. 
Otherwise, given that the $\epsilon$-Greedy strategy's regret scales linearly, the UCB and Thompson sampling algorithms are a better choice, given they are zero-regret strategies and will exploit the best arms in later rounds.

\subsection{Upper Confidence Bound}\label{subsec:upper-confidence-bound2}

\subsubsection{Strengths}
The UCB method quickly finds the optimal arm and continuously expoits it, while the greedy algorithm, although also finding the optimal action relatively quickly, has too much randomness for larger numbers of rounds.
The UCB algorithm does not perform exploration by simply choosing any action selected with a constant probability, but changes its exploration-exploitation balance when collecting more environmental knowledge.
\\The UCB-1 results from a single simulation are very stable and similar to the average results.
This demonstrates that the algorithm can balance exploration and exploitation in a single experiment, can optimize contact allocation to discover the best arm, and then exploit it.
\\ For arbitrary bounded rewards, the KL-UCB algorithm satisfies a uniformly better regret bound than UCB and its variants. Any maximizer can be chosen in this strategy.\citep{Garivier2011}

\subsubsection{Weaknesses}
Many variants need to be considered, for example, the non-stochastic bandit, which makes no assumptions about the reward distribution, cannot be estimated under UCB\@.
\\UCB methods can handle bandits with a small number of arms and high reward differences well, but when K becomes larger, their performance deteriorates faster than other algorithms ~\citep{Kuleshov}.
\\The choice of confidence interval also affects the precision.
If we choose 95 percent rather than 99 percent, it will generate less specific results.
\\UCB1 converges to a solution much slower than the other algorithms, although that final solution appears very good, which is consistent with the reported results~\citep{Auer2002}.
Reinforcement comparison generates good average regret per turn towards the end of 1000 turns for small numbers of arms (K = 2,5), but starts trailing the other algorithms at larger values of K (K = 10, 50).
Its total regret is relatively high because it is slower in the beginning, due to the need to estimate the average reward.
Overall, our results suggest that there is no advantage to using pursuit and reinforcement comparison in practice~\citep{Kuleshov}.

\subsubsection{Best conditions for UCB algorithm}
Provided that we find the best value for our confidence level, the UCB algorithm works about as well as the Thompson sampling algorithm (which is even more computationally expensive), and this is shown by the plots from our simulations. 
Therefore, we can conclude that the UCB algorithm is very useful in practice, although this very much depends on whether we choose the right confidence level, as illustrated by the plots above \ref{alg:ucb_algorithm}; this algorithm works best with a large number of rounds, so the size of its confidence intervals for each arm decrease enough and converge towards the mean for each arm.

\subsection{Thompson Sampling}\label{subsec:thompson-sampling}
Similar to the Upper Confidence Bound (UCB) algorithm above, Thompson Sampling chooses an arm to play based on the highest estimated reward.
Unlike the UCB algorithm, however, the 'highest estimates' in this case depend on the best \textbf{sample} from the conjugate prior distributions (explained in section 2.3) for each arm, rather than the upper bound of the confidence interval around the estimated value for each arm, as the UCB does.

\subsubsection{Strengths}
By sampling from the prior distributions for each arm, rather than picking the best estimate like $\epsilon$-greedy or the best potential estimate like UCB, we get a better proportion of draws, with which we can model our new prior distributions - this gives us a very good exploration-exploitation balance.
In addition, unlike the UCB (which sets a confidence value to control the level of exploration) and $\epsilon$-greedy (which sets an exploration value $\epsilon$) algorithms, a parameter doesn't need to be set for Thompson sampling, meaning that Thompson sampling doesn't depend on how well we choose our parameters.

Furthermore, (although beyond the scope of this project), Gaussian Thompson sampling does not necessarily need to have known precision - if both the mean and precision are unknown, we construct our conjugate priors using the \textbf{Normal-Gamma} distribution, with four parameters.
This demonstrates that even with very little information about the underlying distribution, Thompson sampling still manages to find optimal arms and a zero regret strategy;
this is the case with all distributions which have (known) conjugate priors.

\subsubsection{Weaknesses}
By trying to estimate the underlying distribution for each arm, and continuously updating prior distributions using posterior results, Thompson sampling ends up being very computationally expensive compared to algorithms like $\epsilon$-Greedy~\citep{mazumdar2020thompson}.
Although the computational cost can be decreased using algorithms such as Markov Chain Monte Carlo methods, these will be less accurate as they are numerical approximations~\citep{mazumdar2020thompson}.
In addition, for a smaller number of arms with means close to each other, we see that a strategy such as $\epsilon$-first would work better, as Thompson sampling could still choose to play arms using samples from inferior prior distributions, whereas the $\epsilon$-first strategy will locate the arm with the highest expectation far more quickly.
It should also be noted that the arm which actually has the highest expectation could also give a really low reward for the first round, meaning that the algorithm may not exploit it for many of the following rounds, which is especially bad with a low number of arms and rounds.
Furthermore, in this project, we have only considered arm distributions where the prior and posterior are \textbf{conjugate distributions} - when this is not the case, Thompson sampling becomes even more computationally expensive, as we have to consider different prior and posterior distributions with changing parameters~\citep{zhou2018racing}.
Finally, Thompson sampling requires us to know the form of the underlying distribution of each arm so we can construct prior distributions effectively; 
in practice, however, it is unlikely that we will know these distributions, meaning that other methods (such as UCB or $\epsilon$-first) would work better without this information.

\subsubsection{Best conditions for Thompson sampling algorithm}
From this, we can say that Thompson sampling works best when there are a large number of rounds and the prior distributions for each arm are known, as this would result in more accurate prior distributions for each round over the long run.
A smaller number of arms will also maximize the efficiency of Thompson sampling, as this means there is less time spent exploring each arm's distributions and more time exploiting the arm with the highest estimated mean.
As Thompson sampling is a zero-regret strategy, we can also say that after a large number of rounds, the cumulative regret will cease to increase.


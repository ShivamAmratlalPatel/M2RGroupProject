\section{Analysing Simulations}
We see from above that the $\epsilon$-first strategy isn't necessarily a zero-regret strategy. We see this as the graph for $\epsilon$ = 0.5 doesn't increase in cumulative regret after it finishes its exploration phase, however this is not the case for the two $\epsilon$ values (0.75 and 0.9) which are greater \ref{fig: epsilon}. This demonstrates to us that a higher $\epsilon$ value doesn't mean we are guaranteed to find the arm with the highest actual mean, even with more exploration, due to the fact that we have either had the arm with the highest mean underestimated, or an arm with a lower mean overestimated.

We also see that for our 100 machine plot \ref{fig: all4}, Thompson sampling works the best and is a zero-regret strategy, whilst the random strategy is the worst, with its cumulative regret scaling linearly. Here, UCB also scales to a zero-regret strategy, and $\epsilon$-first does not. However, $\epsilon$-first does better than UCB in the number of trials given, due to a fairly low amount of exploration followed by exploitation ($\epsilon$-value 0.2).